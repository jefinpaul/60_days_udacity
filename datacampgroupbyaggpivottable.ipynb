{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": ".ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnFzroj6KizZ",
        "colab_type": "text"
      },
      "source": [
        "###Counting\n",
        "we ll learn to summarize categorical data using counting, if vets wants to know how many dogs of each breed have visited the vets office\n",
        "<br/> * when calculating we need to avoid same name < drop_duplicates>\n",
        "<br/> * also keeping in mind that the same name with different breed is POSSIBLE <br/>< by specifying the column name>\n",
        " <br/>\n",
        " unique_dogs[\"breed\"].value_counts() \n",
        "  <br/>\n",
        "  in sorting manner <br/>\n",
        " unique_dogs[\"breed\"].value_counts(sort = True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vt-szF8O_9l",
        "colab_type": "text"
      },
      "source": [
        "###Instructions\n",
        "* Remove rows of sales with duplicate pairs of store and type and save as store_types and print the head.\n",
        "<br/>\n",
        "* Remove rows of sales with duplicate pairs of store and department and save as store_depts and print the head.\n",
        "<br/>\n",
        "* Subset the rows that are holiday weeks, and drop the duplicate dates, saving as holiday_dates.\n",
        "<br/>\n",
        "* Select the date column of holiday_dates, and print.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAOxpyRPO8aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop duplicate store/type combinations\n",
        "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
        "print(store_types.head())\n",
        "\n",
        "# Drop duplicate store/department combinations\n",
        "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
        "print(store_depts.head())\n",
        "\n",
        "# Subset the rows that are holiday weeks and drop duplicate dates\n",
        "holiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=\"date\")\n",
        "\n",
        "# Print date col of holiday_dates\n",
        "print(holiday_dates[\"date\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jMthcjylJVe",
        "colab_type": "text"
      },
      "source": [
        "###Instruction\n",
        "* Count the number of stores of each store type.\n",
        "*Count the proportion of stores of each store type.\n",
        "*Count the number of different department numbers, sorting the counts in descending order.\n",
        "*Count the proportion of different department numbers, sorting the proportions in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqQawuG8Pn43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count the number of stores of each type\n",
        "store_counts = stores[\"store_type\"].value_counts()\n",
        "print(store_counts)\n",
        "\n",
        "# Get the proportion of stores of each type\n",
        "store_props = stores[\"store_type\"].value_counts(normalize=True)\n",
        "print(store_props)\n",
        "\n",
        "# Count the number of each department number and sort\n",
        "dept_counts_sorted = departments[\"department_num\"].value_counts(sort=True)\n",
        "print(dept_counts_sorted)\n",
        "\n",
        "# Get the proportion of departments of each number and sort\n",
        "dept_props_sorted = departments[\"department_num\"].value_counts(sort=True, normalize=True)\n",
        "print(dept_props_sorted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlRdMy4RxSnf",
        "colab_type": "text"
      },
      "source": [
        "#### Instruction 3\n",
        "* Calculate the total weekly sales over the whole dataset.\n",
        "* Subset for type \"A\" stores, and calculate their total weekly sales.\n",
        "* Do the same for type \"B\" and type \"C\" stores.\n",
        "* Combine the A/B/C results into a list, and divide by overall sales to get the proportion of sales by type.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz4QplU30JvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to cal the total weekly sales\n",
        "sales_all = sales[\"weekly_sales\"].sum()\n",
        "#to cal the sum of only A column, so first we need to subset that column\n",
        "sales_A = sales[sales[type]==\"A\"][\"weel;y_sales\"].sum()\n",
        "\n",
        "# Subset for type B stores, calc total weekly sales\n",
        "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type C stores, calc total weekly sales\n",
        "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Get proportion for each type\n",
        "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
        "print(sales_propn_by_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgJT9w2C_5Yt",
        "colab_type": "text"
      },
      "source": [
        "next section up is using both groupby and aggregates in which we can use min max median at the same time but dont forget to use np.min etc...cuz numpy is imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwRn8sjdDr--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import NumPy with the alias np\n",
        "import numpy as np\n",
        "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
        "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min,np.max,np.mean,np.median])\n",
        "\n",
        "# Print sales_stats\n",
        "print(sales_stats)\n",
        "\n",
        "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
        "unemp_fuel_stats = sales.groupby(\"type\")[\"unemployment\",\"fuel_price_usd_per_l\"].agg([np.min, np.max, np.mean, np.median])\n",
        "\n",
        "# Print unemp_fuel_stats\n",
        "print(unemp_fuel_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x29_q0ZsE54t",
        "colab_type": "text"
      },
      "source": [
        "##Pivoting on one variable\n",
        "Pivot tables are the standard way of aggregating data in spreadsheets. In pandas, pivot tables are essentially just another way of performing grouped calculations. That is, the .pivot_table() method is just an alternative to .groupby()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lumu6e1Ffdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type.\n",
        "# Pivot for mean weekly_sales for each store type\n",
        "mean_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\")\n",
        "\n",
        "# Print mean_sales_by_type\n",
        "print(mean_sales_by_type)\n",
        "\n",
        "\n",
        "#Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type.\n",
        "# Import NumPy as np\n",
        "import numpy as np\n",
        "\n",
        "# Pivot for mean and median weekly_sales for each store type\n",
        "mean_med_sales_by_type = sales.pivot_table(values= \"weekly_sales\", index=\"type\", aggfunc= [np.mean, np.median])\n",
        "\n",
        "# Print mean_med_sales_by_type\n",
        "print(mean_med_sales_by_type)\n",
        "\n",
        "#Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.\n",
        "#dont forget to use columns after index even though in question it says by type and is_holidays\n",
        "mean_sales_by_type_holiday = sales.pivot_table(values=\"weekly_sales\", index=\"type\", columns=\"is_holiday\")\n",
        "\n",
        "# Print mean_sales_by_type_holiday\n",
        "print(mean_sales_by_type_holiday)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bx3a60HMlu",
        "colab_type": "text"
      },
      "source": [
        "next seesion up is fill by values and that is last session in this lesson by far and summing rowns and columns use margins=true"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbtaRSknHYa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Print the mean weekly_sales by department and type, filling in any missing values with 0 and summing all rows and columns.\n",
        "\n",
        "\n",
        "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0,margins=True))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}